{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 6552418,
          "sourceType": "datasetVersion",
          "datasetId": 3786485
        },
        {
          "sourceId": 2603715,
          "sourceType": "datasetVersion",
          "datasetId": 1582403
        }
      ],
      "dockerImageVersionId": 30627,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Heart Attack Risk - ANN from scratch using np",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "Firstly, we will need to import the necessary libraries to pre-process the data and create a neural network model. We will code a neural network from scratch using NumPy in our first approach. For the second approach we will be using keras built on top of tensorflow to create a model with the same architecture. For the third approach we will test our tensorflow model on another dataset."
      ],
      "metadata": {
        "_uuid": "5540fe5a-0ff4-44df-8da6-1458f4c43792",
        "_cell_guid": "b28d8a35-10b7-4e7b-8fb2-0c3d3651a1a7",
        "trusted": true,
        "id": "RpI47XUiDy9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.utils import plot_model\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "# Set seed for NumPy\n",
        "np.random.seed(42)\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_uuid": "b17e9834-66f0-4f28-abd9-1b34b717b0de",
        "_cell_guid": "9785aec1-2a8a-4d9f-abf0-1596bdb8ee18",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:34.724327Z",
          "iopub.execute_input": "2024-02-15T10:26:34.725314Z",
          "iopub.status.idle": "2024-02-15T10:26:34.893704Z",
          "shell.execute_reply.started": "2024-02-15T10:26:34.725268Z",
          "shell.execute_reply": "2024-02-15T10:26:34.892348Z"
        },
        "trusted": true,
        "id": "7nHsZDMbDy9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = pd.read_csv(\"heart_attack_prediction_dataset.csv\")\n",
        "file.head()"
      ],
      "metadata": {
        "_uuid": "b1add34c-5756-4bdd-aff5-6f1186a913e2",
        "_cell_guid": "ce810695-ff06-4a07-9603-d9af01e7f333",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:34.896295Z",
          "iopub.execute_input": "2024-02-15T10:26:34.896691Z",
          "iopub.status.idle": "2024-02-15T10:26:34.965691Z",
          "shell.execute_reply.started": "2024-02-15T10:26:34.896654Z",
          "shell.execute_reply": "2024-02-15T10:26:34.964803Z"
        },
        "trusted": true,
        "id": "uUAiKhcjDy9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing\n",
        "Before the data can be fed into our model to train, the data needs to be processed. Irrelevant features will be removed and categorical features will be OneHotEncoded."
      ],
      "metadata": {
        "_uuid": "266a635c-9448-4f53-9806-27e7dcb67520",
        "_cell_guid": "ec15cde3-8d31-4134-bc6c-9c2813d3b129",
        "trusted": true,
        "id": "T2h3cdKFDy9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file.shape"
      ],
      "metadata": {
        "_uuid": "dc840845-7154-4b49-a24f-93b692c51927",
        "_cell_guid": "e6232bcb-b865-4168-ad68-7bcfd76b760f",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:34.966783Z",
          "iopub.execute_input": "2024-02-15T10:26:34.967054Z",
          "iopub.status.idle": "2024-02-15T10:26:34.973142Z",
          "shell.execute_reply.started": "2024-02-15T10:26:34.967029Z",
          "shell.execute_reply": "2024-02-15T10:26:34.972048Z"
        },
        "trusted": true,
        "id": "MhjTEK_mDy9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">  The shape of the dataframe is (8763, 26), which means there's 8763 patients and 26 features (columns) per patient. We will look at the columns in the dataframe and select the suitable ones for training."
      ],
      "metadata": {
        "_uuid": "c6330358-ba93-4074-92a8-17c4b69d7642",
        "_cell_guid": "ad9c8748-d559-421b-9a94-75538b4d7785",
        "trusted": true,
        "id": "_3Ueai3nDy9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file.columns #outputs the column names"
      ],
      "metadata": {
        "_uuid": "55f1ab8b-0bae-4967-858e-9f0487c8fa6d",
        "_cell_guid": "ac16f093-7701-49e8-8fa6-f8809440e67e",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:34.97546Z",
          "iopub.execute_input": "2024-02-15T10:26:34.975764Z",
          "iopub.status.idle": "2024-02-15T10:26:34.987325Z",
          "shell.execute_reply.started": "2024-02-15T10:26:34.975738Z",
          "shell.execute_reply": "2024-02-15T10:26:34.986394Z"
        },
        "trusted": true,
        "id": "jCHJz2PrDy9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = file.drop([\"Patient ID\", \"Income\",\"Continent\",\"Hemisphere\",\"Heart Attack Risk\", \"Country\"],axis=1)\n",
        "y = file['Heart Attack Risk']"
      ],
      "metadata": {
        "_uuid": "4508ccd8-ee8a-4720-b330-240e78328883",
        "_cell_guid": "710e0aa1-ba4a-43ba-ba8e-d017d7b012d8",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:34.988293Z",
          "iopub.execute_input": "2024-02-15T10:26:34.988654Z",
          "iopub.status.idle": "2024-02-15T10:26:35.002658Z",
          "shell.execute_reply.started": "2024-02-15T10:26:34.988626Z",
          "shell.execute_reply": "2024-02-15T10:26:35.001569Z"
        },
        "trusted": true,
        "id": "lf-Y2GxhDy9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of y: \", y.shape)\n",
        "y.head()"
      ],
      "metadata": {
        "_uuid": "7b95aa56-1cbc-4959-a93e-c4b2ec112d2f",
        "_cell_guid": "c75a71cf-9d76-4f8b-b266-656401dc29c0",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:35.004004Z",
          "iopub.execute_input": "2024-02-15T10:26:35.004638Z",
          "iopub.status.idle": "2024-02-15T10:26:35.016089Z",
          "shell.execute_reply.started": "2024-02-15T10:26:35.004595Z",
          "shell.execute_reply": "2024-02-15T10:26:35.015202Z"
        },
        "trusted": true,
        "id": "taebnh7yDy9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OneHotEncoding and converting non-numerical data into numerical data\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "f9f52ee5-b387-44de-ab00-d41c2790b42b",
        "_cell_guid": "d3bfee34-b61e-42d8-b806-bebad438f693",
        "trusted": true,
        "id": "twLNF9CcDy9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#OneHotEncoding Diet and Sex\n",
        "\n",
        "x = pd.get_dummies(x, columns=[\"Diet\",\"Sex\"],prefix=[\"Diet\",\"Sex\"])\n",
        "\n",
        "#Seperating the Systolic and Diastolic Blood Pressure and dropping the Blood Pressure column\n",
        "\n",
        "x[[\"Systolic\",\"Diastolic\"]] = x[\"Blood Pressure\"].str.split('/', expand=True)\n",
        "x[['Systolic', 'Diastolic']] = x[['Systolic', 'Diastolic']].apply(pd.to_numeric)\n",
        "x = x.drop('Blood Pressure', axis=1)"
      ],
      "metadata": {
        "_uuid": "d29a8808-6c67-4948-a6bf-c00020222f95",
        "_cell_guid": "205b2656-8808-4df8-a2f5-1c7ab463143d",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:35.017116Z",
          "iopub.execute_input": "2024-02-15T10:26:35.017481Z",
          "iopub.status.idle": "2024-02-15T10:26:35.085369Z",
          "shell.execute_reply.started": "2024-02-15T10:26:35.017447Z",
          "shell.execute_reply": "2024-02-15T10:26:35.084364Z"
        },
        "trusted": true,
        "id": "oGOWjPMlDy9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Now the data has been processed as required. All irrelevant columns have been dropped and all non-numeric values have been converted into numeric values. We can see the processed data below along with it's new shape. It still has 8763 rows but the number of columns have changed since we dropped a few and created a few through OneHotEncoding."
      ],
      "metadata": {
        "_uuid": "55b86b5f-111f-4cac-ada0-2cf94b7edb0c",
        "_cell_guid": "e626bec0-49e3-47fe-bce0-817ff4b7e4b3",
        "trusted": true,
        "id": "ncc_aIGiDy9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape: \", x.shape)\n",
        "x.head()"
      ],
      "metadata": {
        "_uuid": "bdcb3528-cd78-4913-9291-60759d1b11d8",
        "_cell_guid": "1af00705-31bb-4b60-ad3a-6896ed0a5a39",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:35.086466Z",
          "iopub.execute_input": "2024-02-15T10:26:35.086747Z",
          "iopub.status.idle": "2024-02-15T10:26:35.11211Z",
          "shell.execute_reply.started": "2024-02-15T10:26:35.086722Z",
          "shell.execute_reply": "2024-02-15T10:26:35.111161Z"
        },
        "trusted": true,
        "id": "PHcgMIq4Dy9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalisation\n",
        "> There is one last step remaining in pre-processing the data before we can feed it into the neural network. Since the columns have a range of different values, we will carry out normalisation to improve gradient descent. In this step, the data will also be converted from a DataFrame to a numpy array so that it can be easily fed into the neural network."
      ],
      "metadata": {
        "_uuid": "b2f9c946-4dee-4eef-b64c-01da1b661dbb",
        "_cell_guid": "682ef9aa-1dc8-42ed-9421-997357b084ab",
        "trusted": true,
        "id": "V51BDWlYDy9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler=StandardScaler()\n",
        "x=scaler.fit_transform(x)\n",
        "x"
      ],
      "metadata": {
        "_uuid": "4d5f82c2-ab3b-4640-a9c6-c34bd603eb85",
        "_cell_guid": "99c700e7-66e7-4166-b9f9-9364ef44c0e1",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:35.113326Z",
          "iopub.execute_input": "2024-02-15T10:26:35.11395Z",
          "iopub.status.idle": "2024-02-15T10:26:35.137929Z",
          "shell.execute_reply.started": "2024-02-15T10:26:35.113914Z",
          "shell.execute_reply": "2024-02-15T10:26:35.136888Z"
        },
        "trusted": true,
        "id": "WPqbUv_XDy9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a training set and validation set\n",
        "* Since we want our model to generalise, we will be splitting the data into a train set and validation set. The validation set will comprise of 20% of the original data and the rest of the 80% of the data will be used for training. Once training is done using the training set, we can use the validation set to see how well our model has generalised."
      ],
      "metadata": {
        "_uuid": "7271e7d9-8116-415b-9ebd-1b73abe9142f",
        "_cell_guid": "d08e7f26-db31-4f3c-a57b-3991f1711f39",
        "trusted": true,
        "id": "8MUzOGdLDy9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "random_seed = 42 #to get consistent results\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state = random_seed)\n",
        "\n",
        "#Adjusting the shape of y\n",
        "y_train = y_train.values.reshape(-1, 1)\n",
        "y_test = y_test.values.reshape(-1, 1)\n",
        "y_test.shape"
      ],
      "metadata": {
        "_uuid": "d8de3b07-7935-495b-92d7-01d5d06a1d7a",
        "_cell_guid": "5c800785-c217-4860-b614-e8c7e3761c2e",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:35.142202Z",
          "iopub.execute_input": "2024-02-15T10:26:35.142581Z",
          "iopub.status.idle": "2024-02-15T10:26:35.152634Z",
          "shell.execute_reply.started": "2024-02-15T10:26:35.142554Z",
          "shell.execute_reply": "2024-02-15T10:26:35.151615Z"
        },
        "trusted": true,
        "id": "aFw9zjI-Dy9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Approach - Coding a Neural Network from Scratch\n",
        "* Below is the code for the neural network we coded using numpy. The code is divided into several functions for each step which are compiled together in **L_layer_model(X, Y, layers_dims, learning_rate=0.01, num_iterations=30, print_cost=True)** to create a functional model. The model is hard-coded to have **ReLU activation in the hidden layers and Sigmoid activation** in the output layer. The loss is computed using the **compute_cost()** function which computes the loss using **Binary Crossentropy** since we are trying to predict **True** or **False** instead of a probability. The description of each function is given as a comment under the respective function."
      ],
      "metadata": {
        "id": "92iflk_JDy9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an overview of the functions that are used to built the neural network.\n",
        ">* **relu**: Computes the ReLU.\n",
        ">* **relu_backward**: Implements the backward propagation for a single ReLU unit.\n",
        ">* **sigmoid_backward**: Implements the backward propagation for a single sigmoid unit.\n",
        ">* **initialize_parameters_deep**: Initializes the parameters of a deep neural network.\n",
        ">* **linear_forward**: Implements the linear part of a layer's forward propagation.\n",
        ">* **linear_activation_forward**: Implements the forward propagation for the linear -> activation.\n",
        ">* **L_model_forward**: Implements forward propagation for the entire network.\n",
        ">* **compute_cost**: Computes the cost function (cross-entropy cost).\n",
        ">* **linear_backward**: Implements the linear portion of backward propagation for a single layer.\n",
        ">* **linear_activation_backward**: Implements the backward propagation for the linear -> activation.\n",
        ">* **L_model_backward**: Implements the backward propagation for the entire network.\n",
        ">* **update_parameters**: Updates parameters using gradient descent.\n",
        ">* **L_layer_model**: Implements a L-layer neural network. L is the number of layers defined by the user.\n",
        ">* **predict**: Predicts the results of a L-layer neural network.\n",
        ">* **plot_costs**: Plots the learning curve.\n",
        ""
      ],
      "metadata": {
        "id": "9s7O22gtDy9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid activation function element-wise.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- output of the sigmoid function, same shape as Z\n",
        "    cache -- returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "    A = 1 / (1 + np.exp(-Z))\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Compute the ReLU activation function element-wise.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- Output of the linear layer, of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- Post-activation parameter, of the same shape as Z\n",
        "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    A = np.maximum(0, Z)\n",
        "    assert (A.shape == Z.shape)\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True)\n",
        "    dZ[Z <= 0] = 0\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    Z = cache\n",
        "    s = 1 / (1 + np.exp(-Z))\n",
        "    dZ = dA * s * (1 - s)\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    return dZ\n",
        "\n",
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Initialize the parameters of the deep neural network.\n",
        "\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        assert (parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
        "        assert (parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "    return parameters\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter\n",
        "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    Z = W.dot(A) + b\n",
        "    cache = (A, W, b)\n",
        "    return Z, cache\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value\n",
        "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "    cache = (linear_cache, activation_cache)\n",
        "    return A, cache\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "\n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "\n",
        "    Returns:\n",
        "    AL -- activation value from the output (last) layer\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
        "    \"\"\"\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
        "        caches.append(cache)\n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "    return AL, caches\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]\n",
        "    cost = -np.sum(np.multiply(np.log(AL), Y) + np.multiply(np.log(1 - AL), (1 - Y))) / m\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "    dW = np.dot(dZ, A_prev.T) / m\n",
        "    db = (np.sum(dZ, axis=1, keepdims=True)) / m\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l\n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "\n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ...\n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches)\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape)\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    current_cache = caches[L - 1]\n",
        "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
        "    grads[\"dA\" + str(L - 1)] = dA_prev_temp\n",
        "    grads[\"dW\" + str(L)] = dW_temp\n",
        "    grads[\"db\" + str(L)] = db_temp\n",
        "    for l in reversed(range(L - 1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "    return grads\n",
        "\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "\n",
        "    Arguments:\n",
        "    params -- python dictionary containing your parameters\n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "                  parameters[\"W\" + str(l)] = ...\n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    parameters = copy.deepcopy(params)\n",
        "    L = len(parameters) // 2\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
        "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
        "    return parameters\n",
        "\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate=0.01, num_iterations=30, print_cost=True):\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data, of shape (n_x, number of examples)\n",
        "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 5 steps\n",
        "\n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    np.random.seed(1)\n",
        "    costs = []  # keep track of cost\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    for i in range(0, num_iterations):\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        cost = compute_cost(AL, Y)\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "        if i % 100 == 0 or i == num_iterations:\n",
        "            costs.append(cost)\n",
        "    return parameters, costs\n",
        "\n",
        "def predict(X, y, parameters):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "\n",
        "    Arguments:\n",
        "    X -- data set of examples you would like to label\n",
        "    parameters -- parameters of the trained model\n",
        "\n",
        "    Returns:\n",
        "    p -- predictions for the given dataset X\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2\n",
        "    p = np.zeros((1, m))\n",
        "    probas, caches = L_model_forward(X, parameters)\n",
        "    for i in range(0, probas.shape[1]):\n",
        "        if probas[0, i] > 0.5:\n",
        "            p[0, i] = 1\n",
        "        else:\n",
        "            p[0, i] = 0\n",
        "    print(\"Accuracy: \" + str(np.sum((p == y) / m)))\n",
        "    return p\n",
        "\n",
        "def plot_costs(costs, learning_rate=0.0075):\n",
        "    \"\"\"\n",
        "    Plot the learning curve.\n",
        "\n",
        "    Arguments:\n",
        "    costs -- list of costs\n",
        "    learning_rate -- learning rate used during training\n",
        "    \"\"\"\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:35.154267Z",
          "iopub.execute_input": "2024-02-15T10:26:35.154644Z",
          "iopub.status.idle": "2024-02-15T10:26:35.198171Z",
          "shell.execute_reply.started": "2024-02-15T10:26:35.154602Z",
          "shell.execute_reply": "2024-02-15T10:26:35.197346Z"
        },
        "trusted": true,
        "id": "DReoZRXBDy9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Neural Network Architecture\n",
        "* We are going to use Dense layers, which means all the neurons in a layer will be connected to all the neurons in the previous layer.\n",
        "* Using the neural network coded in the previous cell, we can build a custom model by specifying the number of layers and the neurons per layer. This is done in the **layer_dims** list in the following code cell.\n",
        "* Building a model is an iterative process, it is difficult to built the optimum model in the first try. For our baseline model we will use 5 layers (including the input and output layer). The **input layer consists of 24 nodes as there are 24 features**. The **3 hidden layers contain 32, 16 and 8 neurons respectively**. The **output layer contains only 1 neuron**. The architecture strikes a balance between model complexity and capacity. With 5 layers, the network has the capacity to learn intricate patterns and relationships within the data. However, it's not overly complex, which could lead to overfitting, especially in scenarios where the dataset is not sufficiently large.\n",
        "* The input layer has no activation function. The **hidden layers compute ReLU activation** as it will increase the rate of gradient descent and the **output layer computes a sigmoid activation** since it is the best choice for binary classification.\n"
      ],
      "metadata": {
        "_uuid": "39edee81-b723-44bd-afdb-68fae9f2e796",
        "_cell_guid": "cc17d750-331c-4fd2-8af8-b5cf497b9acf",
        "trusted": true,
        "id": "0M7oT8I1Dy9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Layers ###\n",
        "layers_dims = [24, 32, 16, 8, 1] #Specify the number of neurons in each layer"
      ],
      "metadata": {
        "_uuid": "b5f387fc-77da-4eb6-855c-7e5766054dd2",
        "_cell_guid": "fd063708-9e34-446f-948b-ef1cc394f772",
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:35.199187Z",
          "iopub.execute_input": "2024-02-15T10:26:35.199477Z",
          "iopub.status.idle": "2024-02-15T10:26:35.21206Z",
          "shell.execute_reply.started": "2024-02-15T10:26:35.199426Z",
          "shell.execute_reply": "2024-02-15T10:26:35.211078Z"
        },
        "trusted": true,
        "id": "w-FaxLwtDy9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the visual represenation of our neural network."
      ],
      "metadata": {
        "_uuid": "924f501a-a57c-4e3a-b39f-3ac45371300b",
        "_cell_guid": "4f27801c-5ccd-48de-8563-525c5eadb45e",
        "trusted": true,
        "id": "ALLk2jygDy9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Neural Network Architecture](https://svgshare.com/i/133N.svg)"
      ],
      "metadata": {
        "_uuid": "1a941a07-df1a-4088-bb3c-c3a14dd7d4ff",
        "_cell_guid": "00107dab-d620-4bad-905a-5970d2ff5cbf",
        "trusted": true,
        "id": "kGiQoypEDy9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model\n",
        "> After we're done with creating the architecture for the neural network, we can train the model using model.compile.\n",
        "* The learning rate is set to a low value of **0.01**\n",
        "* The **loss** is calculated using **BinaryCrossentropy()**\n",
        "* The network is trained for **1000 epochs**\n",
        "* The learned parameters *(updated weights and biases)* from the training is stored in the **parameters** variable\n",
        "\n",
        "\n",
        "A graph plot is produced to observe the cost against the epochs."
      ],
      "metadata": {
        "id": "j-qrbl78Dy9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "parameters, costs = L_layer_model(x_train.T, y_train.T, layers_dims, num_iterations = 1000, print_cost = True, learning_rate=0.01)\n",
        "#Plot the graph of cost against iteration\n",
        "plot_costs(costs, learning_rate=0.01)"
      ],
      "metadata": {
        "_uuid": "58d13d57-4ae2-4829-8d6d-a96b312f3ab6",
        "_cell_guid": "05b25349-9974-4f49-a11b-5e8e5c131c84",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:35.213379Z",
          "iopub.execute_input": "2024-02-15T10:26:35.213756Z",
          "iopub.status.idle": "2024-02-15T10:26:50.079471Z",
          "shell.execute_reply.started": "2024-02-15T10:26:35.213723Z",
          "shell.execute_reply": "2024-02-15T10:26:50.078507Z"
        },
        "trusted": true,
        "id": "aUmY6mT9Dy9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting with the model"
      ],
      "metadata": {
        "id": "yXvG6TPpDy9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the learned parameters of the model to predict from both the training set and test set"
      ],
      "metadata": {
        "id": "0t-JT-0xDy9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting on the train set\n",
        "predictions_train = predict(x_train.T, y_train.T, parameters)\n",
        "#predicting on the test set\n",
        "predictions_test = predict(x_test.T, y_test.T, parameters)"
      ],
      "metadata": {
        "_uuid": "ea8fae78-72e6-46f7-8ddc-8b5d3443f9b3",
        "_cell_guid": "38bfa0df-d55f-4d28-9cbe-6147e3ee97f1",
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:50.081009Z",
          "iopub.execute_input": "2024-02-15T10:26:50.081407Z",
          "iopub.status.idle": "2024-02-15T10:26:50.112824Z",
          "shell.execute_reply.started": "2024-02-15T10:26:50.081376Z",
          "shell.execute_reply": "2024-02-15T10:26:50.11125Z"
        },
        "trusted": true,
        "id": "C5tyu-n8Dy9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysing the model\n",
        "The accuracy on both the training set and the test set is similar. The model isn't overfitting to the training set but it is also not generalising properly. We are not using any momentum optimisation, thus the loss may haven gotten stuck in a local minima which is why it is not overfitting even after 1000 epochs."
      ],
      "metadata": {
        "id": "cxpmhhSeDy9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Possible reasons for low accuracy\n",
        "> The low accuracy can be due to a number of reasons. Firstly, it can be due to a bad model with suboptimal hyperparameters or due to incorrect feature selection. However, in this case, the most likely reason for the low accuracy is because of the data. According to the publisher of the data, the dataset is a synthetic dataset generated using ChatGPT. Given this, it's understandable that the data may not faithfully represent real-world scenarios or exhibit meaningful patterns. Therefore, it's conceivable that the output accuracy is hindered by the inherent limitations of the dataset quality, adhering to the principle of ***garbage in, garbage out.***"
      ],
      "metadata": {
        "id": "QiZNEHJUDy9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Approach - Building a Model Using Tensorflow Keras\n",
        "In our first approach we built a neural network from scratch to understand how to implement the computations in code. In the second approach, we are going to use Keras on top of tensorflow to train the model on the same dataset. We are opting for a Keras model because it comes in-built with optimisations that will help us run the model faster. It will also allow us to easily tune the hyperparameters using Genetic Algorithm."
      ],
      "metadata": {
        "id": "FADj6iU5Dy9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Tensorflow Model\n",
        "As we are going to use the same dataset from the previous approach, there is no further need to pre-process the data; we can jump straight to building the Keras model.\n",
        "> We are using Keras to build our neural network model since it allows us to easily create the neural network architecture without having to do any math. Using Keras, we can simply just add a layer using Dense() and specify the number of neurons in the layer and the activation function. The architecture of this network will be the same as the first approach.\n",
        "* We are going to use Dense layers, which means all the layers will be connected to all the neurons in the previous layer.\n",
        "* We will use 5 layers (including the input and output layer). The **input layer consists of 24 nodes as there are 24 features**. The **3 hidden layers contain 32, 16 and 8 neurons respectively**. The **output layer contains only 1 neuron**. (The same baseline model as the previous approach)\n",
        "* The input layer has no activation function. The **hidden layers compute ReLU activation** and the **output layer computes a sigmoid activation** since it is the best choice for binary classification.\n"
      ],
      "metadata": {
        "id": "JBEY47d6Dy9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = x.shape[1] #number of features\n",
        "\n",
        "#Creating the neural network model\n",
        "model = Sequential([\n",
        "    tf.keras.Input(shape=(n,), name=\"input_layer\"), #Input Layer with 'n' neurons\n",
        "    Dense(units=32, activation=\"relu\", name=\"hidden_layer1\"),\n",
        "    Dense(units=16, activation=\"relu\", name=\"hidden_layer2\"),\n",
        "    Dense(units=8, activation=\"relu\", name=\"hidden_layer3\"),\n",
        "    Dense(units=1, activation=\"sigmoid\", name=\"output_layer\"),\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:50.114896Z",
          "iopub.execute_input": "2024-02-15T10:26:50.115651Z",
          "iopub.status.idle": "2024-02-15T10:26:50.273839Z",
          "shell.execute_reply.started": "2024-02-15T10:26:50.115598Z",
          "shell.execute_reply": "2024-02-15T10:26:50.272822Z"
        },
        "trusted": true,
        "id": "quKbjZ2uDy9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model\n",
        "> After we're done with creating the architecture for the neural network, we can train the model using model.compile.\n",
        "* We're using the **Adam optimisation** algorithm which makes use of **momentum** and **RMSProp** to smooth out the gradient descent.\n",
        "* The learning rate is set to a low value of **0.01**\n",
        "* The **loss** is calculated using **BinaryCrossentropy()**\n",
        "* The network is trained for **250 epochs** with a **batch size = dataset size**"
      ],
      "metadata": {
        "id": "NhRhifHADy9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.01), loss = tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, batch_size = x_train.shape[0], epochs = 250, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:26:50.275126Z",
          "iopub.execute_input": "2024-02-15T10:26:50.275477Z",
          "iopub.status.idle": "2024-02-15T10:27:01.217536Z",
          "shell.execute_reply.started": "2024-02-15T10:26:50.275447Z",
          "shell.execute_reply": "2024-02-15T10:27:01.216499Z"
        },
        "trusted": true,
        "id": "3dXNWscnDy9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysing the model\n",
        "We can analyse how well our tensorflow model performs by looking at the loss and accuracy plotted against epoch. As we can see from the graphs below, the accuracy when the model predicts from data it has already been trained on (the training set) increases per epoch. However, when the model sees new data (validation set) the accuracy decreases per epoch."
      ],
      "metadata": {
        "id": "F5IVAt4xDy9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "val_acc = history.history['val_accuracy']  # Assuming the metric is named 'val_accuracy'\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.plot(range(len(history.history['accuracy'])), history.history['accuracy'], label='Training Accuracy', marker='o', color='b')\n",
        "\n",
        "# Plot validation accuracy\n",
        "plt.plot(range(len(val_acc)), val_acc, label='Validation Accuracy', marker='s', color='g')\n",
        "\n",
        "# Add labels, title, legend, grid, etc.\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy per Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:27:01.219296Z",
          "iopub.execute_input": "2024-02-15T10:27:01.219665Z",
          "iopub.status.idle": "2024-02-15T10:27:01.691625Z",
          "shell.execute_reply.started": "2024-02-15T10:27:01.219634Z",
          "shell.execute_reply": "2024-02-15T10:27:01.690605Z"
        },
        "trusted": true,
        "id": "OOXLjjglDy9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the graph under the code cell below, we can see the Loss for the training set decreases per epoch, but the loss for validation increases per epoch."
      ],
      "metadata": {
        "id": "bW_qEM05Dy9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "val_acc = history.history['val_loss']\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.plot(range(len(history.history['loss'])), history.history['loss'], label='Training Loss', marker='o', color='b')\n",
        "# Plot validation accuracy\n",
        "plt.plot(range(len(val_acc)), val_acc, label='Validation Loss', marker='s', color='g')\n",
        "\n",
        "# Add labels, title, legend, grid, etc.\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss per Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:27:01.695247Z",
          "iopub.execute_input": "2024-02-15T10:27:01.695589Z",
          "iopub.status.idle": "2024-02-15T10:27:02.006573Z",
          "shell.execute_reply.started": "2024-02-15T10:27:01.695561Z",
          "shell.execute_reply": "2024-02-15T10:27:02.005571Z"
        },
        "trusted": true,
        "id": "a3CJEWE6Dy9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we have the classification report when the model predicts from the training set."
      ],
      "metadata": {
        "id": "eI7QKRNzDy9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply threshold for binary predictions\n",
        "a = x_train\n",
        "b = y_train\n",
        "predictions = model.predict(a)\n",
        "threshold = 0.5\n",
        "binary_predictions = (predictions >= threshold).astype(int)\n",
        "\n",
        "# Evaluate and print results\n",
        "accuracy = accuracy_score(b, binary_predictions)\n",
        "conf_matrix = confusion_matrix(b, binary_predictions)\n",
        "class_report = classification_report(b, binary_predictions)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:27:02.007916Z",
          "iopub.execute_input": "2024-02-15T10:27:02.008295Z",
          "iopub.status.idle": "2024-02-15T10:27:02.62223Z",
          "shell.execute_reply.started": "2024-02-15T10:27:02.008259Z",
          "shell.execute_reply": "2024-02-15T10:27:02.621045Z"
        },
        "trusted": true,
        "id": "tiMpFATvDy9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we have the classification report when the model predicts from the test set."
      ],
      "metadata": {
        "id": "J-Qi8gQkDy9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply threshold for binary predictions\n",
        "a = x_test\n",
        "b = y_test\n",
        "predictions = model.predict(a)\n",
        "threshold = 0.5\n",
        "binary_predictions = (predictions >= threshold).astype(int)\n",
        "\n",
        "# Evaluate and print results\n",
        "accuracy = accuracy_score(b, binary_predictions)\n",
        "conf_matrix = confusion_matrix(b, binary_predictions)\n",
        "class_report = classification_report(b, binary_predictions)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:27:02.623631Z",
          "iopub.execute_input": "2024-02-15T10:27:02.624002Z",
          "iopub.status.idle": "2024-02-15T10:27:02.817551Z",
          "shell.execute_reply.started": "2024-02-15T10:27:02.623968Z",
          "shell.execute_reply": "2024-02-15T10:27:02.816586Z"
        },
        "trusted": true,
        "id": "bC22-TMzDy9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the accuracy against epochs graph, the tensorflow model performs the same on the validation set as the neural network coded from scratch until about 30 epochs. From this we can conclude that the calculations within our coded neural network in the first approach were accurate. However, **the tensorflow model starts overfitting which our first model did not**, even though we ran more epochs in the first model. This is most likely due to the use of **Adam optimisation, which helps the gradient descent to escape a local minima and start fitting well to the training set**. Due to the overfitting the accuracy on the training set increases but the accuracy on the validation set decreases."
      ],
      "metadata": {
        "id": "fC8D_m5-Dy9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Keras neural network using Genetic Algorithm\n",
        "Genetic Algorithm can be used to to tune the hyperparameters of a model by searching for the best combination of parameters such as number of hidden layers, number of neurons, learning rate etc.\n",
        "\n",
        "For our demo we are using GA to omptimise the model by only finding the best combination of neurons in the Hidden Layers. This is a maximisation operation where the best choice is that which produces the model with the most accurate results (i.e. the **accuracy** of the model is the **fitness value**). Since training a genetic algorithm takes a lot of time, we will only be using a **initial population size of 5**, and **3 maximum generations**. The **crossover probability is 0.8** and the **mutation probabilty is 0.2.**\n",
        "\n",
        "As training a Genetic Algorithm is computationally resource intensive, we have opted for a basic implementation here. With enough time and resources, we can test for other hyperparameters as well."
      ],
      "metadata": {
        "id": "3gU5655nDy9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from deap import base, creator, tools\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate(individual):\n",
        "    # Decode the individual to obtain the architecture\n",
        "    n_neurons_hidden1, n_neurons_hidden2, n_neurons_hidden3 = individual\n",
        "    # Construct the neural network\n",
        "    model = Sequential([\n",
        "        tf.keras.Input(shape=(n,), name=\"input_layer\"),\n",
        "        Dense(units=n_neurons_hidden1, activation=\"relu\", name=\"hidden_layer1\"),\n",
        "        Dense(units=n_neurons_hidden2, activation=\"relu\", name=\"hidden_layer2\"),\n",
        "        Dense(units=n_neurons_hidden3, activation=\"relu\", name=\"hidden_layer3\"),\n",
        "        Dense(units=1, activation=\"sigmoid\", name=\"output_layer\"),\n",
        "    ])\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.1), loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "    # Train the model\n",
        "    history = model.fit(x_train, y_train, batch_size=8763, epochs=200, validation_data=(x_test, y_test), verbose=0)\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    # Return negative accuracy as fitness (maximization problem)\n",
        "    return accuracy,\n",
        "\n",
        "# Define genetic algorithm parameters\n",
        "POP_SIZE = 5\n",
        "NUM_GENERATIONS = 3\n",
        "CROSSOVER_PROB = 0.8\n",
        "MUTATION_PROB = 0.2\n",
        "\n",
        "# Create a toolbox for the genetic algorithm\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_int\", np.random.randint, low=1, high=100)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_int, n=3)  # 3 parameters for the number of neurons in each hidden layer\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"evaluate\", evaluate)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutUniformInt, low=1, up=100, indpb=0.2)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "# Generate initial population\n",
        "population = toolbox.population(n=POP_SIZE)\n",
        "\n",
        "# Run the genetic algorithm\n",
        "for gen in range(NUM_GENERATIONS):\n",
        "    print(\"Generation\", gen + 1)\n",
        "\n",
        "    # Evaluate the fitness of the population\n",
        "    fitnesses = list(map(toolbox.evaluate, population))\n",
        "    for ind, fit in zip(population, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    # Print the parents' values and fitness values\n",
        "    print(\"Parents:\")\n",
        "    for ind in population:\n",
        "        print(\"Individual:\", ind, \"Fitness:\", ind.fitness.values[0])\n",
        "\n",
        "    # Select parents for reproduction\n",
        "    offspring = toolbox.select(population, len(population))\n",
        "    offspring = list(map(toolbox.clone, offspring))\n",
        "\n",
        "    # Apply crossover and mutation\n",
        "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "        if np.random.rand() < CROSSOVER_PROB:\n",
        "            toolbox.mate(child1, child2)\n",
        "            del child1.fitness.values\n",
        "            del child2.fitness.values\n",
        "\n",
        "    for mutant in offspring:\n",
        "        if np.random.rand() < MUTATION_PROB:\n",
        "            toolbox.mutate(mutant)\n",
        "            del mutant.fitness.values\n",
        "\n",
        "    # Print the children's values\n",
        "    print(\"Children:\")\n",
        "    for ind in offspring:\n",
        "        print(\"Individual:\", ind)\n",
        "\n",
        "    # Replace the population with the offspring\n",
        "    population[:] = offspring\n",
        "\n",
        "# Get the best individual\n",
        "best_individual = tools.selBest(population, k=1)[0]\n",
        "print(\"Best individual:\", best_individual)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:27:02.819156Z",
          "iopub.execute_input": "2024-02-15T10:27:02.81976Z",
          "iopub.status.idle": "2024-02-15T10:29:05.898566Z",
          "shell.execute_reply.started": "2024-02-15T10:27:02.819719Z",
          "shell.execute_reply": "2024-02-15T10:29:05.897499Z"
        },
        "trusted": true,
        "id": "BDymdK8ODy9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training for 3 iterations, the genetic algorithm gives us the best combination of neurons per hidden layer. However, as we can see from the fitness function of this combination, it is still only around 64% accurate. This low accuracy can be attributed to the quality of the data instead of the quality of the model."
      ],
      "metadata": {
        "id": "PGBwN7V9Dy9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Third Approach - Testing the Neural Network Models on a Different Dataset\n",
        "We have noticed in the previous two approaches that our neural networks did not perform well on the dataset it was trained on. The possible reason for the poor performance is the low quality of the synthetic dataset. Therefore, to demonstrate that our neural network models are working properly, we are going to train and test them on a different dataset.\n",
        "[Click here to see the new dataset we are going to use](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)\n",
        "![](https://i.ibb.co/jk5k7Bb/Screenshot-2024-02-14-211214.png)"
      ],
      "metadata": {
        "id": "mZ-Nl6fYDy9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Pre-Processing\n",
        "Since the new dataset has slightly different features from the previous dataset, we will need to pre-process the data using the same steps we did the first time."
      ],
      "metadata": {
        "id": "bJxN-SDCDy9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"heart.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:29:05.90006Z",
          "iopub.execute_input": "2024-02-15T10:29:05.900463Z",
          "iopub.status.idle": "2024-02-15T10:29:05.923618Z",
          "shell.execute_reply.started": "2024-02-15T10:29:05.900424Z",
          "shell.execute_reply": "2024-02-15T10:29:05.922602Z"
        },
        "trusted": true,
        "id": "4IZhSI5tDy9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One Hot Encoding\n",
        "df = pd.get_dummies(df, columns=[\"Sex\",\"ChestPainType\",\"RestingECG\",\"ExerciseAngina\",\"ST_Slope\"],prefix=[\"Sex\",\"ChestPainType\",\"RestingECG\",\"ExerciseAngina\",\"ST_Slope\"])\n",
        "\n",
        "#Splitting the input and the output\n",
        "X_new = df.drop([\"HeartDisease\"], axis = 1)\n",
        "y_new = df[\"HeartDisease\"]\n",
        "\n",
        "#Normalisation of the data and splitting into train and test set\n",
        "X_new = scaler.fit_transform(X_new)\n",
        "x_train_new, x_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.2, random_state = 42)\n",
        "\n",
        "#Adjusting the shape of y\n",
        "y_train_new = y_train_new.values.reshape(-1, 1)\n",
        "y_test_new = y_test_new.values.reshape(-1, 1)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:29:05.925132Z",
          "iopub.execute_input": "2024-02-15T10:29:05.925449Z",
          "iopub.status.idle": "2024-02-15T10:29:05.948432Z",
          "shell.execute_reply.started": "2024-02-15T10:29:05.925422Z",
          "shell.execute_reply": "2024-02-15T10:29:05.947587Z"
        },
        "trusted": true,
        "id": "j_rTGoSsDy9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_new.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:29:05.949842Z",
          "iopub.execute_input": "2024-02-15T10:29:05.950187Z",
          "iopub.status.idle": "2024-02-15T10:29:05.956754Z",
          "shell.execute_reply.started": "2024-02-15T10:29:05.950154Z",
          "shell.execute_reply": "2024-02-15T10:29:05.955755Z"
        },
        "trusted": true,
        "id": "ZVPpZDufDy9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Keras Model\n",
        "We will be using the same baseline model as before with 5 layers, however the numper of neurons in the input layer will be different as we have different number of features in this new dataset"
      ],
      "metadata": {
        "id": "1xkFkwwxDy9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_new = X_new.shape[1] #number of features\n",
        "\n",
        "#Creating the neural network model\n",
        "model_new = Sequential([\n",
        "    tf.keras.Input(shape=(n_new,), name=\"input_layer\"), #Input Layer with 'n' neurons\n",
        "    Dense(units=32, activation=\"relu\", name=\"hidden_layer1\"),\n",
        "    Dense(units=16, activation=\"relu\", name=\"hidden_layer2\"),\n",
        "    Dense(units=8, activation=\"relu\", name=\"hidden_layer3\"),\n",
        "    Dense(units=1, activation=\"sigmoid\", name=\"output_layer\"),\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:29:05.958182Z",
          "iopub.execute_input": "2024-02-15T10:29:05.958584Z",
          "iopub.status.idle": "2024-02-15T10:29:06.024346Z",
          "shell.execute_reply.started": "2024-02-15T10:29:05.958546Z",
          "shell.execute_reply": "2024-02-15T10:29:06.023503Z"
        },
        "trusted": true,
        "id": "0G7c_HXJDy9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Keras Model\n",
        "* Learning rate = 0.001\n",
        "* Batch size = 64\n",
        "* Epochs = 100"
      ],
      "metadata": {
        "id": "icCj1EkIDy9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_new.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001), loss = tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
        "history = model_new.fit(x_train_new, y_train_new, batch_size = 64, epochs = 100, validation_data=(x_test_new, y_test_new))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:29:06.025609Z",
          "iopub.execute_input": "2024-02-15T10:29:06.025888Z",
          "iopub.status.idle": "2024-02-15T10:29:15.217155Z",
          "shell.execute_reply.started": "2024-02-15T10:29:06.025863Z",
          "shell.execute_reply": "2024-02-15T10:29:15.216377Z"
        },
        "trusted": true,
        "id": "vYwJ7PodDy9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysing the model"
      ],
      "metadata": {
        "id": "xY5ugrkaDy9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "val_acc = history.history['val_accuracy']  # Assuming the metric is named 'val_accuracy'\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.plot(range(len(history.history['accuracy'])), history.history['accuracy'], label='Training Accuracy', marker='o', color='b')\n",
        "\n",
        "# Plot validation accuracy\n",
        "plt.plot(range(len(val_acc)), val_acc, label='Validation Accuracy', marker='s', color='g')\n",
        "\n",
        "# Add labels, title, legend, grid, etc.\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy per Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:29:15.21872Z",
          "iopub.execute_input": "2024-02-15T10:29:15.219107Z",
          "iopub.status.idle": "2024-02-15T10:29:15.515705Z",
          "shell.execute_reply.started": "2024-02-15T10:29:15.21907Z",
          "shell.execute_reply": "2024-02-15T10:29:15.514736Z"
        },
        "trusted": true,
        "id": "4IMqWQqBDy9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As observed from the graph above, the model has performed exceptionally well on the new dataset. Instead of the accuracies of the training set and validation set diverging by a large margin, both of the accuracies improved per epoch. While the model slightly overfits to the training set, it has generalised well to the validation set with an accuracy of almost 90%"
      ],
      "metadata": {
        "id": "ohLGPEUQDy9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimising the new model using Genetic Algorithm\n",
        "Like we did in our second approach, we are going to try to optimise this model using Genetic Algorithm by trying to find the optimum number of neurons per layer. In future practice, we can use genetic algorithm to tune the other hyperparameters as well."
      ],
      "metadata": {
        "id": "rmG0KY30Dy9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from deap import base, creator, tools\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate(individual):\n",
        "    # Decode the individual to obtain the architecture\n",
        "    n_neurons_hidden1, n_neurons_hidden2, n_neurons_hidden3 = individual\n",
        "    # Construct the neural network\n",
        "    model = Sequential([\n",
        "        tf.keras.Input(shape=(n_new,), name=\"input_layer\"),\n",
        "        Dense(units=n_neurons_hidden1, activation=\"relu\", name=\"hidden_layer1\"),\n",
        "        Dense(units=n_neurons_hidden2, activation=\"relu\", name=\"hidden_layer2\"),\n",
        "        Dense(units=n_neurons_hidden3, activation=\"relu\", name=\"hidden_layer3\"),\n",
        "        Dense(units=1, activation=\"sigmoid\", name=\"output_layer\"),\n",
        "    ])\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.01), loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "    # Train the model\n",
        "    history = model.fit(x_train_new, y_train_new, batch_size=128, epochs=50, validation_data=(x_test_new, y_test_new), verbose=0)\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(x_test_new, y_test_new, verbose=0)\n",
        "    # Return negative accuracy as fitness (maximization problem)\n",
        "    return accuracy,\n",
        "\n",
        "# Define genetic algorithm parameters\n",
        "POP_SIZE = 8\n",
        "NUM_GENERATIONS = 3\n",
        "CROSSOVER_PROB = 1.0\n",
        "MUTATION_PROB = 0.2\n",
        "\n",
        "# Create a toolbox for the genetic algorithm\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_int\", np.random.randint, low=1, high=100)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_int, n=3)  # 3 parameters for the number of neurons in each hidden layer\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"evaluate\", evaluate)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutUniformInt, low=1, up=100, indpb=0.2)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "# Generate initial population\n",
        "population = toolbox.population(n=POP_SIZE)\n",
        "\n",
        "# Run the genetic algorithm\n",
        "for gen in range(NUM_GENERATIONS):\n",
        "    print(\"Generation\", gen + 1)\n",
        "\n",
        "    # Evaluate the fitness of the population\n",
        "    fitnesses = list(map(toolbox.evaluate, population))\n",
        "    for ind, fit in zip(population, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    # Print the parents' values and fitness values\n",
        "    print(\"Parents:\")\n",
        "    for ind in population:\n",
        "        print(\"Individual:\", ind, \"Fitness:\", ind.fitness.values[0])\n",
        "\n",
        "    # Select parents for reproduction\n",
        "    offspring = toolbox.select(population, len(population))\n",
        "    offspring = list(map(toolbox.clone, offspring))\n",
        "\n",
        "    # Apply crossover and mutation\n",
        "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "        if np.random.rand() < CROSSOVER_PROB:\n",
        "            toolbox.mate(child1, child2)\n",
        "            del child1.fitness.values\n",
        "            del child2.fitness.values\n",
        "\n",
        "    for mutant in offspring:\n",
        "        if np.random.rand() < MUTATION_PROB:\n",
        "            toolbox.mutate(mutant)\n",
        "            del mutant.fitness.values\n",
        "\n",
        "    # Print the children's values\n",
        "    print(\"Children:\")\n",
        "    for ind in offspring:\n",
        "        print(\"Individual:\", ind)\n",
        "\n",
        "    # Replace the population with the offspring\n",
        "    population[:] = offspring\n",
        "\n",
        "# Get the best individual\n",
        "best_individual = tools.selBest(population, k=1)[0]\n",
        "print(\"Best individual:\", best_individual)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:29:15.520051Z",
          "iopub.execute_input": "2024-02-15T10:29:15.520361Z",
          "iopub.status.idle": "2024-02-15T10:30:58.758516Z",
          "shell.execute_reply.started": "2024-02-15T10:29:15.520331Z",
          "shell.execute_reply": "2024-02-15T10:30:58.757377Z"
        },
        "trusted": true,
        "id": "qfB-qhKgDy9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from training the model with this new dataset, the model performs much better. Both the validation and training accuracy increase per epoch. However, after about 40 epochs, the model starts overfitting to the training data and thus the validation and training accuracy starts diverging. This was trained only for 100 epochs and yet it performed better, so from this we can conclude that the model failed to learn the patterns from the previous dataset is because the dataset is of low quality. The genetic algorithm is likely to improve the model a lot more if a it is run for more generations with a bigger population size and more hyperparameters are tested."
      ],
      "metadata": {
        "id": "DOFvTVnmDy9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Neural Network coded from scratch on the new dataset\n",
        "We've seen our tensorflow model perform much better on the new dataset. Now we will see how our neural network used in the first approach performs."
      ],
      "metadata": {
        "id": "HQXxLD8-Dy9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Layers ###\n",
        "layers_dims_new = [20, 32, 16, 8, 1] #Specify the number of neurons in each layer\n",
        "#Train the model\n",
        "parameters_new, costs_new = L_layer_model(x_train_new.T, y_train_new.T, layers_dims_new, num_iterations = 2000, print_cost = True, learning_rate=0.01)\n",
        "#Plot the graph of cost against iteration\n",
        "plot_costs(costs_new, learning_rate=0.01)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:30:58.759889Z",
          "iopub.execute_input": "2024-02-15T10:30:58.760183Z",
          "iopub.status.idle": "2024-02-15T10:31:04.490316Z",
          "shell.execute_reply.started": "2024-02-15T10:30:58.760156Z",
          "shell.execute_reply": "2024-02-15T10:31:04.489449Z"
        },
        "trusted": true,
        "id": "36w35R1yDy9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the results, the cost tapers off and stops decreasing after some iterations. This is most likely because of a gradient vanishing problem where the gradient descent is stuck in a local minima. Since we did not use any optimisation such as momentum for this network, the model is not escaping the local minima. It is possible that if we run the model for a lot more iterations, it will be able to break out from this local minimum and fall into the global basin."
      ],
      "metadata": {
        "id": "HfVY5B8ODy9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting on the train set\n",
        "predictions_train_new = predict(x_train_new.T, y_train_new.T, parameters_new)\n",
        "#predicting on the test set\n",
        "predictions_test_new = predict(x_test_new.T, y_test_new.T, parameters_new)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-15T10:31:04.491616Z",
          "iopub.execute_input": "2024-02-15T10:31:04.491978Z",
          "iopub.status.idle": "2024-02-15T10:31:04.509842Z",
          "shell.execute_reply.started": "2024-02-15T10:31:04.491943Z",
          "shell.execute_reply": "2024-02-15T10:31:04.508349Z"
        },
        "trusted": true,
        "id": "MUgtmqjCDy9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the model is stuck in a local minima, the accuracy of the model is quite low."
      ],
      "metadata": {
        "id": "2Pa56NhCDy9S"
      }
    }
  ]
}